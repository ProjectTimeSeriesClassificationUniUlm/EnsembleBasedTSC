{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from Helpers import get_confusion_matrix_for_model_and_data\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from TrainProdecure import train_single_model\n",
    "from ModelBuilder import get_FCN\n",
    "from LoadData import get_all_datasets_test_train_np_arrays\n",
    "\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from LoadData import get_fcn_datasets_test_train_np_arrays, DATASETS_FCN\n",
    "\n",
    "EPOCHS_PER_TRAINING = 20\n",
    "# USED_DATASETS = get_fcn_datasets_test_train_np_arrays('../datasets/')\n",
    "USED_DATASETS = get_all_datasets_test_train_np_arrays('../datasets/', ds_names=DATASETS_FCN[:3])\n",
    "GENERATE_MODEL = get_FCN\n",
    "MODEL_NAME = 'FCN'\n",
    "FINE_TUNE_LEARNING_RATE = 10e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train_model_from_scratch(x_train, y_train, x_test, y_test, dataset_name):\n",
    "    \"\"\"\n",
    "    Function to train a model from scratch on a given dataset\n",
    "    :return: the trained model, the loss, the accuracy, and the history\n",
    "    \"\"\"\n",
    "    input_size = x_train.shape[1]\n",
    "    output_size = len(np.unique(y_train))\n",
    "\n",
    "    scratch_model = GENERATE_MODEL(input_size, output_size)\n",
    "    # scratch_model.summary()\n",
    "\n",
    "    return train_single_model(\n",
    "        scratch_model, x_train, y_train, x_test, y_test,\n",
    "        epochs=EPOCHS_PER_TRAINING, batch_size=None, model_name=MODEL_NAME, dataset_name=dataset_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def transfer_learning_model_one_dataset(x_train, y_train, x_test, y_test, dataset_name):\n",
    "    input_size = x_train.shape[1]\n",
    "    output_size = len(np.unique(y_train))\n",
    "\n",
    "    model = GENERATE_MODEL(input_size, output_size)\n",
    "\n",
    "    return train_single_model(\n",
    "        model, x_train, y_train, x_test, y_test,\n",
    "        epochs=EPOCHS_PER_TRAINING, batch_size=25, model_name='FCN',\n",
    "        dataset_name=dataset_name\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def two_confusion_matrices(confusion_matrix_1, model_name_1, confusion_matrix_2, model_name_2, dataset_name):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.title(f'Transfer Learning for {dataset_name}')\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.heatmap(confusion_matrix_1, annot=True, fmt=\"d\")\n",
    "    plt.title(f\"Confusion Matrix for {model_name_1}\")\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.heatmap(confusion_matrix_2, annot=True, fmt=\"d\")\n",
    "    plt.title(f\"Confusion Matrix for {model_name_2}\")\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def transfer_learning_for_all_datasets(used_datasets):\n",
    "    \"\"\"\n",
    "    Function to train a model from scratch on a given dataset\n",
    "    :return: the trained model, the loss, the accuracy, and the history\n",
    "    \"\"\"\n",
    "    # Target ds for which it is fine-tuned:\n",
    "    for dataset_name, data_dict in tqdm(used_datasets.items(), unit='datasets', desc='Transfer Learning for all datasets'):\n",
    "        print(f\"Training model targeting dataset {dataset_name}\")\n",
    "\n",
    "        x_train, y_train = data_dict['train_data']\n",
    "        x_test, y_test = data_dict['test_data']\n",
    "\n",
    "        # ds for pre-training:\n",
    "        datasets_subset = {name: value for name, value in used_datasets.items() if name != dataset_name}\n",
    "        for pre_train_ds_name, pre_train_data_dict in tqdm(datasets_subset.items(), unit='datasets', desc='Pre-training on different datasets'):\n",
    "            print(f\"Pre-training model on dataset {dataset_name}\")\n",
    "\n",
    "            pre_train_x_train, pre_train_y_train = pre_train_data_dict['train_data']\n",
    "            pre_train_x_test, pre_train_y_test = pre_train_data_dict['test_data']\n",
    "\n",
    "            # Training a model from scratch as a baseline:\n",
    "            scratch_model, scratch_loss, scratch_accuracy, scratch_history = train_model_from_scratch(\n",
    "                x_train, y_train, x_test, y_test, dataset_name\n",
    "            )\n",
    "\n",
    "            # Pre-training a model:\n",
    "            pre_trained_model, _, _, _ = transfer_learning_model_one_dataset(\n",
    "                pre_train_x_train, pre_train_y_train, pre_train_x_test, pre_train_y_test, pre_train_ds_name\n",
    "            )\n",
    "\n",
    "            # Fine-tuning:\n",
    "            input_size = x_train.shape[1]\n",
    "            output_size = len(np.unique(y_train))\n",
    "            headless_model_layers = pre_trained_model.layers[:-1]\n",
    "            fine_tuned_model = keras.Sequential([*headless_model_layers, keras.layers.Dense(output_size, activation='softmax')])\n",
    "            fine_tuned_model.build(input_shape=(None, input_size, 1))\n",
    "            train_single_model(\n",
    "                fine_tuned_model, x_train, y_train, x_test, y_test,\n",
    "                epochs=EPOCHS_PER_TRAINING, batch_size=25, model_name='FCN', dataset_name=dataset_name\n",
    "            )\n",
    "\n",
    "            # Compare confusion matrices:\n",
    "            from_scratch_confusion_matrix = get_confusion_matrix_for_model_and_data(scratch_model, x_test, y_test)\n",
    "            transfer_learned_confusion_matrix = get_confusion_matrix_for_model_and_data(pre_trained_model, x_test, y_test)\n",
    "\n",
    "            two_confusion_matrices(\n",
    "                from_scratch_confusion_matrix, 'Model trained from scratch',\n",
    "                transfer_learned_confusion_matrix, f'Model pre-trained on {pre_train_ds_name}',\n",
    "                dataset_name\n",
    "            )\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "Transfer Learning for all datasets:   0%|          | 0/3 [00:00<?, ?datasets/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "374e11e6998448058900e7d42bc7940e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model targeting dataset cbf\n"
     ]
    },
    {
     "data": {
      "text/plain": "Pre-training on different datasets:   0%|          | 0/2 [00:00<?, ?datasets/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7a2baa51a5524cdbb60c93049637be60"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training model on dataset cbf\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training FCN on cbf dataset: 0epoch [00:00, ?epoch/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e8e15af7222d4559aa67bad1a4a18e6f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 3ms/step - loss: 0.9076 - accuracy: 0.6933\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training FCN on distal_phalanax_tw dataset: 0epoch [00:00, ?epoch/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1ae5e63854954ca7bb9a323fac1fe698"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 6ms/step - loss: 8.8902 - accuracy: 0.1295\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training FCN on cbf dataset: 0epoch [00:00, ?epoch/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "14b85329a5e94ed090fcd471ed4c3f33"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\timwi\\Documents\\Uni\\EnsembleBasedTSC\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\timwi\\Documents\\Uni\\EnsembleBasedTSC\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\timwi\\Documents\\Uni\\EnsembleBasedTSC\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\timwi\\Documents\\Uni\\EnsembleBasedTSC\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\timwi\\Documents\\Uni\\EnsembleBasedTSC\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\timwi\\Documents\\Uni\\EnsembleBasedTSC\\venv\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_2\" is incompatible with the layer: expected shape=(None, 80, 1), found shape=(None, 128)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtransfer_learning_for_all_datasets\u001B[49m\u001B[43m(\u001B[49m\u001B[43mUSED_DATASETS\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[6], line 37\u001B[0m, in \u001B[0;36mtransfer_learning_for_all_datasets\u001B[1;34m(used_datasets)\u001B[0m\n\u001B[0;32m     35\u001B[0m fine_tuned_model \u001B[38;5;241m=\u001B[39m keras\u001B[38;5;241m.\u001B[39mSequential([\u001B[38;5;241m*\u001B[39mheadless_model_layers, keras\u001B[38;5;241m.\u001B[39mlayers\u001B[38;5;241m.\u001B[39mDense(output_size, activation\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msoftmax\u001B[39m\u001B[38;5;124m'\u001B[39m)])\n\u001B[0;32m     36\u001B[0m fine_tuned_model\u001B[38;5;241m.\u001B[39mbuild(input_shape\u001B[38;5;241m=\u001B[39m(\u001B[38;5;28;01mNone\u001B[39;00m, input_size, \u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m---> 37\u001B[0m \u001B[43mtrain_single_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     38\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfine_tuned_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_test\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     39\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mEPOCHS_PER_TRAINING\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m25\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mFCN\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataset_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdataset_name\u001B[49m\n\u001B[0;32m     40\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     42\u001B[0m \u001B[38;5;66;03m# Compare confusion matrices:\u001B[39;00m\n\u001B[0;32m     43\u001B[0m from_scratch_confusion_matrix \u001B[38;5;241m=\u001B[39m get_confusion_matrix_for_model_and_data(scratch_model, x_test, y_test)\n",
      "File \u001B[1;32m~\\Documents\\Uni\\EnsembleBasedTSC\\src\\TrainProdecure.py:35\u001B[0m, in \u001B[0;36mtrain_single_model\u001B[1;34m(model, x_train, y_train, x_test, y_test, epochs, learning_rate, batch_size, validation_split, model_name, dataset_name, optimizer)\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     31\u001B[0m     model\u001B[38;5;241m.\u001B[39mcompile(optimizer\u001B[38;5;241m=\u001B[39moptimizer(),\n\u001B[0;32m     32\u001B[0m                   loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msparse_categorical_crossentropy\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     33\u001B[0m                   metrics\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m---> 35\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     36\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mvalidation_split\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_split\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     37\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mTqdmCallback\u001B[49m\u001B[43m(\u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdesc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mTraining \u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mmodel_name\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m on \u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mdataset_name\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m dataset\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     38\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m x_test \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m y_test \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     41\u001B[0m     test_loss, test_acc \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mevaluate(x_test, y_test)\n",
      "File \u001B[1;32m~\\Documents\\Uni\\EnsembleBasedTSC\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filemxthsrv6.py:15\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001B[1;34m(iterator)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     14\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m---> 15\u001B[0m     retval_ \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(step_function), (ag__\u001B[38;5;241m.\u001B[39mld(\u001B[38;5;28mself\u001B[39m), ag__\u001B[38;5;241m.\u001B[39mld(iterator)), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[0;32m     17\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[1;31mValueError\u001B[0m: in user code:\n\n    File \"C:\\Users\\timwi\\Documents\\Uni\\EnsembleBasedTSC\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\timwi\\Documents\\Uni\\EnsembleBasedTSC\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\timwi\\Documents\\Uni\\EnsembleBasedTSC\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\timwi\\Documents\\Uni\\EnsembleBasedTSC\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\timwi\\Documents\\Uni\\EnsembleBasedTSC\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\timwi\\Documents\\Uni\\EnsembleBasedTSC\\venv\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_2\" is incompatible with the layer: expected shape=(None, 80, 1), found shape=(None, 128)\n"
     ]
    }
   ],
   "source": [
    "transfer_learning_for_all_datasets(USED_DATASETS)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "x_train, y_train = USED_DATASETS['distal_phalanax_tw']['train_data']\n",
    "x_test, y_test = USED_DATASETS['distal_phalanax_tw']['test_data']\n",
    "\n",
    "pre_train_x_train, pre_train_y_train = USED_DATASETS['gun_point_male_female']['train_data']\n",
    "pre_train_x_test, pre_train_y_test = USED_DATASETS['gun_point_male_female']['test_data']\n",
    "\n",
    "dataset_name = 'distal_phalanax_tw'\n",
    "pre_train_ds_name = 'gun_point_male_female'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "Training FCN on distal_phalanax_tw dataset: 0epoch [00:00, ?epoch/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7cf4de949bdb4b2cafaa47107afad28c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 3ms/step - loss: 6.4912 - accuracy: 0.3022\n"
     ]
    }
   ],
   "source": [
    "# Training a model from scratch as a baseline:\n",
    "scratch_model, scratch_loss, scratch_accuracy, scratch_history = train_model_from_scratch(\n",
    "    x_train, y_train, x_test, y_test, dataset_name\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "Training FCN on gun_point_male_female dataset: 0epoch [00:00, ?epoch/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8c8323fca2754d71be545c17013fe49f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 3ms/step - loss: 1.4875 - accuracy: 0.5918\n"
     ]
    }
   ],
   "source": [
    "# Pre-training a model:\n",
    "pre_trained_model, _, _, _ = transfer_learning_model_one_dataset(\n",
    "    pre_train_x_train, pre_train_y_train, pre_train_x_test, pre_train_y_test, pre_train_ds_name\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_30\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_24 (Conv1D)          (None, 150, 128)          1152      \n",
      "                                                                 \n",
      " batch_normalization_24 (Bat  (None, 150, 128)         512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_24 (Activation)  (None, 150, 128)          0         \n",
      "                                                                 \n",
      " conv1d_25 (Conv1D)          (None, 150, 256)          164096    \n",
      "                                                                 \n",
      " batch_normalization_25 (Bat  (None, 150, 256)         1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_25 (Activation)  (None, 150, 256)          0         \n",
      "                                                                 \n",
      " conv1d_26 (Conv1D)          (None, 150, 128)          98432     \n",
      "                                                                 \n",
      " batch_normalization_26 (Bat  (None, 150, 128)         512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_26 (Activation)  (None, 150, 128)          0         \n",
      "                                                                 \n",
      " global_average_pooling1d_6   (None, 128)              0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 6)                 774       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,502\n",
      "Trainable params: 265,478\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training FCN on distal_phalanax_tw dataset: 0epoch [00:00, ?epoch/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "410d4b42447443dd8e96f1e8fe953c00"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\timwi\\Documents\\Uni\\EnsembleBasedTSC\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\timwi\\Documents\\Uni\\EnsembleBasedTSC\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\timwi\\Documents\\Uni\\EnsembleBasedTSC\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\timwi\\Documents\\Uni\\EnsembleBasedTSC\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\timwi\\Documents\\Uni\\EnsembleBasedTSC\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\timwi\\Documents\\Uni\\EnsembleBasedTSC\\venv\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_30\" is incompatible with the layer: expected shape=(None, 150, 1), found shape=(None, 80)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[49], line 8\u001B[0m\n\u001B[0;32m      6\u001B[0m fine_tuned_model\u001B[38;5;241m.\u001B[39msummary()\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# fine_tuned_model.build(input_shape=(None, input_size, 1))\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m \u001B[43mtrain_single_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfine_tuned_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_test\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mEPOCHS_PER_TRAINING\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m25\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mFCN\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataset_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdataset_name\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\Uni\\EnsembleBasedTSC\\src\\TrainProdecure.py:35\u001B[0m, in \u001B[0;36mtrain_single_model\u001B[1;34m(model, x_train, y_train, x_test, y_test, epochs, learning_rate, batch_size, validation_split, model_name, dataset_name, optimizer)\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     31\u001B[0m     model\u001B[38;5;241m.\u001B[39mcompile(optimizer\u001B[38;5;241m=\u001B[39moptimizer(),\n\u001B[0;32m     32\u001B[0m                   loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msparse_categorical_crossentropy\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     33\u001B[0m                   metrics\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m---> 35\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     36\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mvalidation_split\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalidation_split\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     37\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mTqdmCallback\u001B[49m\u001B[43m(\u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdesc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mTraining \u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mmodel_name\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m on \u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mdataset_name\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m dataset\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     38\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m x_test \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m y_test \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     41\u001B[0m     test_loss, test_acc \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mevaluate(x_test, y_test)\n",
      "File \u001B[1;32m~\\Documents\\Uni\\EnsembleBasedTSC\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filemxthsrv6.py:15\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001B[1;34m(iterator)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     14\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m---> 15\u001B[0m     retval_ \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(step_function), (ag__\u001B[38;5;241m.\u001B[39mld(\u001B[38;5;28mself\u001B[39m), ag__\u001B[38;5;241m.\u001B[39mld(iterator)), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[0;32m     17\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[1;31mValueError\u001B[0m: in user code:\n\n    File \"C:\\Users\\timwi\\Documents\\Uni\\EnsembleBasedTSC\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\timwi\\Documents\\Uni\\EnsembleBasedTSC\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\timwi\\Documents\\Uni\\EnsembleBasedTSC\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\timwi\\Documents\\Uni\\EnsembleBasedTSC\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\timwi\\Documents\\Uni\\EnsembleBasedTSC\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\timwi\\Documents\\Uni\\EnsembleBasedTSC\\venv\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_30\" is incompatible with the layer: expected shape=(None, 150, 1), found shape=(None, 80)\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning:\n",
    "input_size = x_train.shape[1]\n",
    "output_size = len(np.unique(y_train))\n",
    "headless_model_layers = pre_trained_model.layers[:-1]\n",
    "fine_tuned_model = keras.Sequential([*headless_model_layers, keras.layers.Dense(output_size, activation='softmax')])\n",
    "fine_tuned_model.summary()\n",
    "# fine_tuned_model.build(input_shape=(None, input_size, 1))\n",
    "train_single_model(\n",
    "    fine_tuned_model, x_train, y_train, x_test, y_test,\n",
    "    epochs=EPOCHS_PER_TRAINING, batch_size=25, model_name='FCN', dataset_name=dataset_name\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "(None, 128)"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tuned_model.output_shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "def get_FCN2(input_size, output_size):\n",
    "    \"\"\"\n",
    "    Create a Fully Convolutional Model based on\n",
    "    Z. Wang, W. Yan, and T. Oates, “Time series classification from scratch with deep neural networks: A strong baseline”, 2017\n",
    "    :param input_size: number of features of the input data\n",
    "    :param output_size: number of classes\n",
    "    :return: keras sequential model of the FCN\n",
    "    \"\"\"\n",
    "    return keras.Sequential([\n",
    "        keras.layers.Conv1D(filters=128, kernel_size=8, padding='same', input_shape=(input_size, 1), kernel_initializer=tf.keras.initializers.GlorotUniform()),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Activation('relu'),\n",
    "\n",
    "        keras.layers.Conv1D(filters=256, kernel_size=5, padding='same', kernel_initializer=tf.keras.initializers.GlorotUniform()),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Activation('relu'),\n",
    "\n",
    "        keras.layers.Conv1D(filters=128, kernel_size=3, padding='same', kernel_initializer=tf.keras.initializers.GlorotUniform()),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Activation('relu'),\n",
    "\n",
    "        keras.layers.GlobalAveragePooling1D(),\n",
    "        # keras.layers.Dense(output_size, activation='softmax', kernel_initializer=tf.keras.initializers.GlorotUniform()),\n",
    "    ])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "(None, 128)"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = get_FCN2(11223, output_size)\n",
    "m.build(input_shape=(None, input_size, 1))\n",
    "m.output_shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compare confusion matrices:\n",
    "from_scratch_confusion_matrix = get_confusion_matrix_for_model_and_data(scratch_model, x_test, y_test)\n",
    "transfer_learned_confusion_matrix = get_confusion_matrix_for_model_and_data(pre_trained_model, x_test, y_test)\n",
    "\n",
    "two_confusion_matrices(\n",
    "    from_scratch_confusion_matrix, 'Model trained from scratch',\n",
    "    transfer_learned_confusion_matrix, f'Model pre-trained on {pre_train_ds_name}',\n",
    "    dataset_name\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
